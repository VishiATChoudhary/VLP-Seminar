{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Notebook for Zero-Shot Inference with CheXzero\n",
    "This notebook walks through how to use CheXzero to perform zero-shot inference on a chest x-ray image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\cheXzeroCode')\n",
    "\n",
    "from eval import evaluate, bootstrap\n",
    "from zero_shot import make, make_true_labels, run_softmax_eval\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheXpert_data_dir =  r\"D:\\CheXpert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints\\\\best_128_0.0002_original_15000_0.859.pt', 'C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints\\\\best_128_0.0002_original_8000_0.857.pt', 'C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints\\\\best_128_5e-05_original_22000_0.855.pt', 'C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints\\\\best_64_0.0001_original_16000_0.861.pt', 'C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints\\\\best_64_0.0001_original_17000_0.863.pt', 'C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints\\\\best_64_0.0001_original_35000_0.864.pt', 'C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints\\\\best_64_0.0002_original_23000_0.854.pt', 'C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints\\\\best_64_5e-05_original_16000_0.858.pt', 'C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints\\\\best_64_5e-05_original_18000_0.862.pt', 'C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints\\\\best_64_5e-05_original_22000_0.864.pt']\n"
     ]
    }
   ],
   "source": [
    "## Define Zero Shot Labels and Templates\n",
    "\n",
    "# ----- DIRECTORIES ------ #\n",
    "cxr_filepath: str = r'C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\cheXzeroCode\\cxr.h5' # filepath of chest x-ray images (.h5)\n",
    "cxr_true_labels_path: Optional[str] = 'D:/CheXpert/groundtruth.csv' # (optional for evaluation) if labels are provided, provide path\n",
    "model_dir: str = 'C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints' # where pretrained models are saved (.pt) \n",
    "predictions_dir: Path = Path('../predictions') # where to save predictions\n",
    "cache_dir: str = predictions_dir / \"cached\" # where to cache ensembled predictions\n",
    "\n",
    "context_length: int = 77\n",
    "\n",
    "# ------- LABELS ------  #\n",
    "# Define labels to query each image | will return a prediction for each label\n",
    "cxr_labels: List[str] = ['Atelectasis','Cardiomegaly', \n",
    "                                      'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "                                      'Lung Opacity', 'No Finding','Pleural Effusion', 'Pleural Other', 'Pneumonia', \n",
    "                                      'Pneumothorax', 'Support Devices']\n",
    "\n",
    "# ---- TEMPLATES ----- # \n",
    "# Define set of templates | see Figure 1 for more details  \n",
    "\n",
    "pathology = \"Cardiomegaly\"                      \n",
    "cxr_pair_template: Tuple[str] = (f\"{pathology}\", f\"no {pathology}\")\n",
    "\n",
    "# ----- MODEL PATHS ------ #\n",
    "# If using ensemble, collect all model paths\n",
    "model_paths = []\n",
    "# print(os.walk(model_dir).__getattribute__)\n",
    "for subdir, dirs, files in os.walk(model_dir):\n",
    "    for file in files:\n",
    "        full_dir = os.path.join(subdir, file)\n",
    "        model_paths.append(full_dir)\n",
    "        \n",
    "print(model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #save all frontal images to a new folder frontal\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define paths\n",
    "# base_folder = r\"D:\\CheXpert\\valid\"  # Replace with the actual path\n",
    "# output_folder = r\"valid_frontal\"  # Replace with the actual path\n",
    "\n",
    "# # Create the output folder if it doesn't exist\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # Copy all frontal images to the output folder\n",
    "# counter = 0\n",
    "# for root, dirs, files in os.walk(base_folder):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\"frontal.jpg\"):\n",
    "\n",
    "#             # get grandparent folder name\n",
    "#             parent_folder = os.path.basename(os.path.dirname(root))\n",
    "#             print(parent_folder)\n",
    "#             newName = f\"{parent_folder}_frontal_{counter}.jpg\"\n",
    "#             img_path = os.path.join(output_folder, newName)\n",
    "#             shutil.copy(os.path.join(root, file), img_path)\n",
    "#             counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert all images in C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\notebooks\\valid_frontal from .jpg to .png\n",
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# # Define paths\n",
    "# base_folder = r\"C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\notebooks\\valid_frontal\"  # Replace with the actual path\n",
    "\n",
    "# # Convert all images to .png\n",
    "# for root, dirs, files in os.walk(base_folder):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".jpg\"):\n",
    "#             img_path = os.path.join(root, file)\n",
    "#             img = Image.open(img_path)\n",
    "#             img.save(img_path.replace(\".jpg\", \".png\"))\n",
    "#             os.remove(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get list of image paths in C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\notebooks\\valid_frontal\n",
    "# import os\n",
    "\n",
    "# # Define paths\n",
    "# base_folder = r\"C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\notebooks\\valid_frontal\"  # Replace with the actual path\n",
    "\n",
    "# # Get all image paths\n",
    "# image_paths = []\n",
    "# for root, dirs, files in os.walk(base_folder):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".png\"):\n",
    "#             image_paths.append(os.path.join(root, file))\n",
    "\n",
    "# image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocess_padchest import img_to_h5\n",
    "\n",
    "# proper_paths = img_to_h5(image_paths, r\"valid.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cxr_filepath\n",
    "\n",
    "# # If using HDF5 file directly\n",
    "# with h5py.File(cxr_filepath, 'r') as f:\n",
    "#     dataset = f['cxr']\n",
    "#     dataset = torch.tensor(dataset)\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Normalize, Resize, InterpolationMode\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, auc, roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import clip\n",
    "from model import CLIP\n",
    "from eval import evaluate, plot_roc, accuracy, sigmoid, bootstrap, compute_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_softmax_eval(model, loader, eval_labels: list, pair_template: tuple, context_length: int = 77): \n",
    "    \"\"\"\n",
    "    Run softmax evaluation to obtain a single prediction from the model.\n",
    "    \"\"\"\n",
    "     # get pos and neg phrases\n",
    "    pos = pair_template[0]\n",
    "    neg = pair_template[1]\n",
    "\n",
    "    # get pos and neg predictions, (num_samples, num_classes)\n",
    "    pos_pred = run_single_prediction(eval_labels, pos, model, loader, \n",
    "                                     softmax_eval=True, context_length=context_length) \n",
    "    neg_pred = run_single_prediction(eval_labels, neg, model, loader, \n",
    "                                     softmax_eval=True, context_length=context_length) \n",
    "\n",
    "    # compute probabilities with softmax\n",
    "    sum_pred = np.exp(pos_pred) + np.exp(neg_pred)\n",
    "    y_pred = np.exp(pos_pred) / sum_pred\n",
    "    return y_pred\n",
    "def run_single_prediction(cxr_labels, template, model, loader, softmax_eval=True, context_length=77): \n",
    "    \"\"\"\n",
    "    FUNCTION: run_single_prediction\n",
    "    --------------------------------------\n",
    "    This function will make probability predictions for a single template\n",
    "    (i.e. \"has {}\"). \n",
    "    \n",
    "    args: \n",
    "        * cxr_labels - list, labels for a specific zero-shot task. (i.e. ['Atelectasis',...])\n",
    "        * template - string, template to input into model. \n",
    "        * model - PyTorch model, trained clip model\n",
    "        * loader - PyTorch data loader, loads in cxr images\n",
    "        * softmax_eval (optional) - Use +/- softmax method for evaluation \n",
    "        * context_length (optional) - int, max number of tokens of text inputted into the model.\n",
    "        \n",
    "    Returns list, predictions from the given template. \n",
    "    \"\"\"\n",
    "    cxr_phrase = [template]\n",
    "    zeroshot_weights = zeroshot_classifier(cxr_labels, cxr_phrase, model, context_length=context_length)\n",
    "    y_pred = predict(loader, model, zeroshot_weights, softmax_eval=softmax_eval)\n",
    "    return y_pred\n",
    "\n",
    "def zeroshot_classifier(classnames, templates, model, context_length=77):\n",
    "    \"\"\"\n",
    "    FUNCTION: zeroshot_classifier\n",
    "    -------------------------------------\n",
    "    This function outputs the weights for each of the classes based on the \n",
    "    output of the trained clip model text transformer. \n",
    "    \n",
    "    args: \n",
    "    * classnames - Python list of classes for a specific zero-shot task. (i.e. ['Atelectasis',...]).\n",
    "    * templates - Python list of phrases that will be indpendently tested as input to the clip model.\n",
    "    * model - Pytorch model, full trained clip model.\n",
    "    * context_length (optional) - int, max number of tokens of text inputted into the model.\n",
    "    \n",
    "    Returns PyTorch Tensor, output of the text encoder given templates. \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        # compute embedding through model for each class\n",
    "        for classname in tqdm(classnames):\n",
    "            texts = [template.format(classname) for template in templates] # format with class\n",
    "            texts = clip.tokenize(texts, context_length=context_length) # tokenize\n",
    "            class_embeddings = model.encode_text(texts) # embed with text encoder\n",
    "            \n",
    "            # normalize class_embeddings\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            # average over templates \n",
    "            class_embedding = class_embeddings.mean(dim=0) \n",
    "            # norm over new averaged templates\n",
    "            class_embedding /= class_embedding.norm() \n",
    "            zeroshot_weights.append(class_embedding)\n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1)\n",
    "    return zeroshot_weights\n",
    "\n",
    "def predict(loader, model, zeroshot_weights, softmax_eval=True, verbose=0): \n",
    "    \"\"\"\n",
    "    FUNCTION: predict\n",
    "    ---------------------------------\n",
    "    This function runs the cxr images through the model \n",
    "    and computes the cosine similarities between the images\n",
    "    and the text embeddings. \n",
    "    \n",
    "    args: \n",
    "        * loader -  PyTorch data loader, loads in cxr images\n",
    "        * model - PyTorch model, trained clip model \n",
    "        * zeroshot_weights - PyTorch Tensor, outputs of text encoder for labels\n",
    "        * softmax_eval (optional) - Use +/- softmax method for evaluation \n",
    "        * verbose (optional) - bool, If True, will print out intermediate tensor values for debugging.\n",
    "        \n",
    "    Returns numpy array, predictions on all test data samples. \n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(loader)):\n",
    "            images = data['img']\n",
    "            # predict\n",
    "            image_features = model.encode_image(images) \n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True) # (1, 768)\n",
    "\n",
    "            # obtain logits\n",
    "            logits = image_features @ zeroshot_weights # (1, num_classes)\n",
    "            logits = np.squeeze(logits.numpy(), axis=0) # (num_classes,)\n",
    "        \n",
    "            if softmax_eval is False: \n",
    "                norm_logits = (logits - logits.mean()) / (logits.std())\n",
    "                logits = sigmoid(norm_logits) \n",
    "            \n",
    "            y_pred.append(logits)\n",
    "            \n",
    "            if verbose: \n",
    "                plt.imshow(images[0][0])\n",
    "                plt.show()\n",
    "                print('images: ', images)\n",
    "                print('images size: ', images.size())\n",
    "                \n",
    "                print('image_features size: ', image_features.size())\n",
    "                print('logits: ', logits)\n",
    "                print('logits size: ', logits.size())\n",
    "         \n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edd19bf727b4bdb812c98be8f7af27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f5c842e50a470586f35fbae166cc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de5cca12ed34c40834fdcc4a7ce5df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e23d19055d4746909b64fdd78a65a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49669266 0.49669266 0.49669266 ... 0.49669266 0.49669266 0.49669266]\n",
      " [0.47938722 0.47938722 0.47938722 ... 0.47938722 0.47938722 0.47938722]\n",
      " [0.50140756 0.50140756 0.50140756 ... 0.50140756 0.50140756 0.50140756]\n",
      " ...\n",
      " [0.49823594 0.49823594 0.49823594 ... 0.49823594 0.49823594 0.49823594]\n",
      " [0.493617   0.493617   0.493617   ... 0.493617   0.493617   0.493617  ]\n",
      " [0.4934756  0.4934756  0.4934756  ... 0.4934756  0.4934756  0.4934756 ]]\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8e3e69bab74c02a9ef33c207d98ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272a3c817f4b4cf1a45ed87bea4cd349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d354d7fa9845b8aa79bb61fe7c7b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296b41d9d81b486fa20c592ef6c4b4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49367982 0.49367982 0.49367982 ... 0.49367982 0.49367982 0.49367982]\n",
      " [0.47942883 0.47942883 0.47942883 ... 0.47942883 0.47942883 0.47942883]\n",
      " [0.49341822 0.49341822 0.49341822 ... 0.49341822 0.49341822 0.49341822]\n",
      " ...\n",
      " [0.49777967 0.49777967 0.49777967 ... 0.49777967 0.49777967 0.49777967]\n",
      " [0.49365163 0.49365163 0.49365163 ... 0.49365163 0.49365163 0.49365163]\n",
      " [0.49385822 0.49385822 0.49385822 ... 0.49385822 0.49385822 0.49385822]]\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9598304abf5f4802868deee015b8fc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08721337300e42649b3ec4f1083f8cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895e84d2ed684481bc6672be00c40e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3450b80d68e4e098882c543d546cee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49494374 0.49494374 0.49494374 ... 0.49494374 0.49494374 0.49494374]\n",
      " [0.4832281  0.4832281  0.4832281  ... 0.4832281  0.4832281  0.4832281 ]\n",
      " [0.49605557 0.49605557 0.49605557 ... 0.49605557 0.49605557 0.49605557]\n",
      " ...\n",
      " [0.49806365 0.49806365 0.49806365 ... 0.49806365 0.49806365 0.49806365]\n",
      " [0.4974348  0.4974348  0.4974348  ... 0.4974348  0.4974348  0.4974348 ]\n",
      " [0.49561802 0.49561802 0.49561802 ... 0.49561802 0.49561802 0.49561802]]\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37a64b37e294f2b950dd90d32532b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97d9821bc6748408c14fa8f9638728d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750d9faa6ea04e6db9af8cadb270c42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fca1b00c1d4bafb5c841c57ec8fc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49659798 0.49659798 0.49659798 ... 0.49659798 0.49659798 0.49659798]\n",
      " [0.4796747  0.4796747  0.4796747  ... 0.4796747  0.4796747  0.4796747 ]\n",
      " [0.49847782 0.49847782 0.49847782 ... 0.49847782 0.49847782 0.49847782]\n",
      " ...\n",
      " [0.4968924  0.4968924  0.4968924  ... 0.4968924  0.4968924  0.4968924 ]\n",
      " [0.49445632 0.49445632 0.49445632 ... 0.49445632 0.49445632 0.49445632]\n",
      " [0.49640775 0.49640775 0.49640775 ... 0.49640775 0.49640775 0.49640775]]\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af27aaf7fed48beb5c58609d90f32a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cb970429154841992556705de769fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4e06d7c06843eb8842a796195f5a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d7fa0556a846a29f83c0c5cef647d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49586436 0.49586436 0.49586436 ... 0.49586436 0.49586436 0.49586436]\n",
      " [0.482375   0.482375   0.482375   ... 0.482375   0.482375   0.482375  ]\n",
      " [0.49594077 0.49594077 0.49594077 ... 0.49594077 0.49594077 0.49594077]\n",
      " ...\n",
      " [0.4964551  0.4964551  0.4964551  ... 0.4964551  0.4964551  0.4964551 ]\n",
      " [0.4943145  0.4943145  0.4943145  ... 0.4943145  0.4943145  0.4943145 ]\n",
      " [0.49776024 0.49776024 0.49776024 ... 0.49776024 0.49776024 0.49776024]]\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c653a7448c834d2c9a0a9d65bfe41745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74414cfc0ddf4ae8a45cad3021c7e070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811bea5efb364f0da27e8e6f4b906f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922071649daa430aa769000eb7ce2b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49701983 0.49701983 0.49701983 ... 0.49701983 0.49701983 0.49701983]\n",
      " [0.47708073 0.47708073 0.47708073 ... 0.47708073 0.47708073 0.47708073]\n",
      " [0.5010132  0.5010132  0.5010132  ... 0.5010132  0.5010132  0.5010132 ]\n",
      " ...\n",
      " [0.5002176  0.5002176  0.5002176  ... 0.5002176  0.5002176  0.5002176 ]\n",
      " [0.49573162 0.49573162 0.49573162 ... 0.49573162 0.49573162 0.49573162]\n",
      " [0.50030744 0.50030744 0.50030744 ... 0.50030744 0.50030744 0.50030744]]\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2229e79c8a94d0a9307f4d4d91623df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff840309af0428183bb2f4cd398452f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd282489a5342f183dcec11a9c7048f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe41925633540b7a0407d9895229f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4920471  0.4920471  0.4920471  ... 0.4920471  0.4920471  0.4920471 ]\n",
      " [0.46385118 0.46385118 0.46385118 ... 0.46385118 0.46385118 0.46385118]\n",
      " [0.49372217 0.49372217 0.49372217 ... 0.49372217 0.49372217 0.49372217]\n",
      " ...\n",
      " [0.4971428  0.4971428  0.4971428  ... 0.4971428  0.4971428  0.4971428 ]\n",
      " [0.4972922  0.4972922  0.4972922  ... 0.4972922  0.4972922  0.4972922 ]\n",
      " [0.5000235  0.5000235  0.5000235  ... 0.5000235  0.5000235  0.5000235 ]]\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b01e03ee1c4f68941d1a1525ce6af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed899d54e7b4dedab9c9c241013b5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dcbef14cbc40e39ddfc1cc279fe49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7180018ecf54498ab0e2e3216677530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49504519 0.49504519 0.49504519 ... 0.49504519 0.49504519 0.49504519]\n",
      " [0.4818718  0.4818718  0.4818718  ... 0.4818718  0.4818718  0.4818718 ]\n",
      " [0.49244052 0.49244052 0.49244052 ... 0.49244052 0.49244052 0.49244052]\n",
      " ...\n",
      " [0.49284372 0.49284372 0.49284372 ... 0.49284372 0.49284372 0.49284372]\n",
      " [0.4936385  0.4936385  0.4936385  ... 0.4936385  0.4936385  0.4936385 ]\n",
      " [0.4949374  0.4949374  0.4949374  ... 0.4949374  0.4949374  0.4949374 ]]\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a8d42f003f45d181dbc92aea270b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fa0a08eac24fb7985deb70e59dbf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd2abcbed734b98aad9cc67458ae871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acc764d5c024242917ebe18e9774150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49608117 0.49608117 0.49608117 ... 0.49608117 0.49608117 0.49608117]\n",
      " [0.48189777 0.48189777 0.48189777 ... 0.48189777 0.48189777 0.48189777]\n",
      " [0.49423403 0.49423403 0.49423403 ... 0.49423403 0.49423403 0.49423403]\n",
      " ...\n",
      " [0.49740708 0.49740708 0.49740708 ... 0.49740708 0.49740708 0.49740708]\n",
      " [0.4966029  0.4966029  0.4966029  ... 0.4966029  0.4966029  0.4966029 ]\n",
      " [0.49293804 0.49293804 0.49293804 ... 0.49293804 0.49293804 0.49293804]]\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b5bb448de2473bad0fa2cc8997867f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d4eebde5334e5fa419d9ab77bba9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc37c76b48084c7eb066549f48b9e5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac1c196a20749428309c72444163bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49674878 0.49674878 0.49674878 ... 0.49674878 0.49674878 0.49674878]\n",
      " [0.48555338 0.48555338 0.48555338 ... 0.48555338 0.48555338 0.48555338]\n",
      " [0.49606162 0.49606162 0.49606162 ... 0.49606162 0.49606162 0.49606162]\n",
      " ...\n",
      " [0.4972743  0.4972743  0.4972743  ... 0.4972743  0.4972743  0.4972743 ]\n",
      " [0.4977789  0.4977789  0.4977789  ... 0.4977789  0.4977789  0.4977789 ]\n",
      " [0.4932244  0.4932244  0.4932244  ... 0.4932244  0.4932244  0.4932244 ]]\n"
     ]
    }
   ],
   "source": [
    "## Run the model on the data set using ensembled models\n",
    "def ensemble_models(\n",
    "    model_paths: List[str], \n",
    "    cxr_filepath: str, \n",
    "    cxr_labels: List[str], \n",
    "    cxr_pair_template: Tuple[str], \n",
    "    cache_dir: str = None, \n",
    "    save_name: str = None,\n",
    ") -> Tuple[List[np.ndarray], np.ndarray]: \n",
    "    \"\"\"\n",
    "    Given a list of `model_paths`, ensemble model and return\n",
    "    predictions. Caches predictions at `cache_dir` if location provided.\n",
    "\n",
    "    Returns a list of each model's predictions and the averaged\n",
    "    set of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    model_paths = sorted(model_paths) # ensure consistency of \n",
    "    for path in model_paths: # for each model\n",
    "        model_name = Path(path).stem\n",
    "\n",
    "        # load in model and `torch.DataLoader`\n",
    "        model, loader = make(\n",
    "            model_path=path, \n",
    "            cxr_filepath=cxr_filepath, \n",
    "        )\n",
    "        # print(type(loader))  # Should print <class 'torch.utils.data.dataloader.DataLoader'>\n",
    "        # print(type(loader.dataset)) \n",
    "        # print(len(loader))\n",
    "        dataset = loader.dataset\n",
    "        dataset = dataset.img_dset\n",
    "        #print dataset attributes\n",
    "        print(type(dataset))\n",
    "        #get type of dataset content\n",
    "        if cache_dir is not None:\n",
    "            if save_name is not None: \n",
    "                cache_path = Path(cache_dir) / f\"{save_name}_{model_name}.npy\"\n",
    "            else: \n",
    "                cache_path = Path(cache_dir) / f\"{model_name}.npy\"\n",
    "\n",
    "        # if prediction already cached, don't recompute prediction\n",
    "        if False:#cache_dir is not None and os.path.exists(cache_path): \n",
    "            print(\"Loading cached prediction for {}\".format(model_name))\n",
    "            y_pred = np.load(cache_path)\n",
    "        else: # cached prediction not found, compute preds\n",
    "            # print(\"Inferring model {}\".format(path))\n",
    "            # print(\"Model name: {}\".format(model_name))\n",
    "            # print(cxr_labels)\n",
    "            # print(cxr_pair_template)\n",
    "            y_pred = run_softmax_eval(model, loader, cxr_labels, cxr_pair_template)\n",
    "            print(y_pred)\n",
    "            if cache_dir is not None: \n",
    "                Path(cache_dir).mkdir(exist_ok=True, parents=True)\n",
    "                np.save(file=cache_path, arr=y_pred)\n",
    "        predictions.append(y_pred)\n",
    "    \n",
    "    # compute average predictions\n",
    "    y_pred_avg = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return predictions, y_pred_avg\n",
    "\n",
    "predictions, y_pred_avg = ensemble_models(\n",
    "    model_paths=model_paths, \n",
    "    cxr_filepath=cxr_filepath, \n",
    "    cxr_labels=cxr_labels, \n",
    "    cxr_pair_template=cxr_pair_template, \n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save averaged preds\n",
    "pred_name = \"chexpert_preds.npy\" # add name of preds\n",
    "predictions_dir = predictions_dir / pred_name\n",
    "np.save(file=predictions_dir, arr=y_pred_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Evaluate Results\n",
    "If ground truth labels are available, compute AUC on each pathology to evaluate the performance of the zero-shot model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4954721 , 0.4954721 , 0.4954721 , ..., 0.4954721 , 0.4954721 ,\n",
       "        0.4954721 ],\n",
       "       [0.47943488, 0.47943488, 0.47943488, ..., 0.47943488, 0.47943488,\n",
       "        0.47943488],\n",
       "       [0.49627715, 0.49627715, 0.49627715, ..., 0.49627715, 0.49627715,\n",
       "        0.49627715],\n",
       "       ...,\n",
       "       [0.49723125, 0.49723125, 0.49723125, ..., 0.49723125, 0.49723125,\n",
       "        0.49723125],\n",
       "       [0.49545184, 0.49545184, 0.49545184, ..., 0.49545184, 0.49545184,\n",
       "        0.49545184],\n",
       "       [0.49585503, 0.49585503, 0.49585503, ..., 0.49585503, 0.49585503,\n",
       "        0.49585503]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [500, 200]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m test_pred\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# evaluate model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m cxr_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcxr_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# boostrap evaluations for 95% confidence intervals\u001b[39;00m\n\u001b[0;32m     10\u001b[0m bootstrap_results \u001b[38;5;241m=\u001b[39m bootstrap(test_pred, test_true, cxr_labels)\n",
      "File \u001b[1;32m~\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\cheXzeroCode\\eval.py:148\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(y_pred, y_true, cxr_labels, roc_name, pr_name, label_idx_map)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' ROC CURVE '''\u001b[39;00m\n\u001b[0;32m    147\u001b[0m roc_name \u001b[38;5;241m=\u001b[39m cxr_label \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ROC Curve\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m fpr, tpr, thresholds, roc_auc \u001b[38;5;241m=\u001b[39m \u001b[43mplot_roc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroc_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m sens, spec \u001b[38;5;241m=\u001b[39m choose_operating_point(fpr, tpr, thresholds)\n\u001b[0;32m    152\u001b[0m results \u001b[38;5;241m=\u001b[39m [[roc_auc]]\n",
      "File \u001b[1;32m~\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\cheXzeroCode\\eval.py:60\u001b[0m, in \u001b[0;36mplot_roc\u001b[1;34m(y_pred, y_true, roc_name, plot)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_roc\u001b[39m(y_pred, y_true, roc_name, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# given the test_ground_truth, and test_predictions \u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     roc_auc \u001b[38;5;241m=\u001b[39m auc(fpr, tpr)\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m plot: \n",
      "File \u001b[1;32mc:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1142\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1041\u001b[0m     {\n\u001b[0;32m   1042\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1051\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m ):\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1142\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m   1154\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:816\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m--> 816\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    817\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n\u001b[0;32m    818\u001b[0m y_score \u001b[38;5;241m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[1;32mc:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [500, 200]"
     ]
    }
   ],
   "source": [
    "# make test_true\n",
    "test_pred = y_pred_avg\n",
    "test_true = make_true_labels(cxr_true_labels_path=cxr_true_labels_path, cxr_labels=cxr_labels)\n",
    "\n",
    "test_pred\n",
    "# evaluate model\n",
    "cxr_results = evaluate(test_pred, test_true, cxr_labels)\n",
    "\n",
    "# boostrap evaluations for 95% confidence intervals\n",
    "bootstrap_results = bootstrap(test_pred, test_true, cxr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Atelectasis_auc</th>\n",
       "      <th>Cardiomegaly_auc</th>\n",
       "      <th>Consolidation_auc</th>\n",
       "      <th>Edema_auc</th>\n",
       "      <th>Enlarged Cardiomediastinum_auc</th>\n",
       "      <th>Fracture_auc</th>\n",
       "      <th>Lung Lesion_auc</th>\n",
       "      <th>Lung Opacity_auc</th>\n",
       "      <th>No Finding_auc</th>\n",
       "      <th>Pleural Effusion_auc</th>\n",
       "      <th>Pleural Other_auc</th>\n",
       "      <th>Pneumonia_auc</th>\n",
       "      <th>Pneumothorax_auc</th>\n",
       "      <th>Support Devices_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.8118</td>\n",
       "      <td>0.9132</td>\n",
       "      <td>0.8901</td>\n",
       "      <td>0.8994</td>\n",
       "      <td>0.9160</td>\n",
       "      <td>0.5603</td>\n",
       "      <td>0.7360</td>\n",
       "      <td>0.9213</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>0.9317</td>\n",
       "      <td>0.6025</td>\n",
       "      <td>0.7798</td>\n",
       "      <td>0.6520</td>\n",
       "      <td>0.7735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower</th>\n",
       "      <td>0.7720</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>0.8201</td>\n",
       "      <td>0.8662</td>\n",
       "      <td>0.8912</td>\n",
       "      <td>0.2646</td>\n",
       "      <td>0.5658</td>\n",
       "      <td>0.8961</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>0.4608</td>\n",
       "      <td>0.5695</td>\n",
       "      <td>0.4854</td>\n",
       "      <td>0.7310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper</th>\n",
       "      <td>0.8479</td>\n",
       "      <td>0.9367</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>0.9295</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.8725</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>0.9426</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.9536</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.9483</td>\n",
       "      <td>0.8243</td>\n",
       "      <td>0.8130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Atelectasis_auc  Cardiomegaly_auc  Consolidation_auc  Edema_auc  \\\n",
       "mean            0.8118            0.9132             0.8901     0.8994   \n",
       "lower           0.7720            0.8849             0.8201     0.8662   \n",
       "upper           0.8479            0.9367             0.9470     0.9295   \n",
       "\n",
       "       Enlarged Cardiomediastinum_auc  Fracture_auc  Lung Lesion_auc  \\\n",
       "mean                           0.9160        0.5603           0.7360   \n",
       "lower                          0.8912        0.2646           0.5658   \n",
       "upper                          0.9375        0.8725           0.8779   \n",
       "\n",
       "       Lung Opacity_auc  No Finding_auc  Pleural Effusion_auc  \\\n",
       "mean             0.9213          0.0700                0.9317   \n",
       "lower            0.8961          0.0451                0.9053   \n",
       "upper            0.9426          0.0952                0.9536   \n",
       "\n",
       "       Pleural Other_auc  Pneumonia_auc  Pneumothorax_auc  Support Devices_auc  \n",
       "mean              0.6025         0.7798            0.6520               0.7735  \n",
       "lower             0.4608         0.5695            0.4854               0.7310  \n",
       "upper             0.8855         0.9483            0.8243               0.8130  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display AUC with confidence intervals\n",
    "bootstrap_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
