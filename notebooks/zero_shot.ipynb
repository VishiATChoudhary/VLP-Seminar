{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Notebook for Zero-Shot Inference with CheXzero\n",
    "This notebook walks through how to use CheXzero to perform zero-shot inference on a chest x-ray image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\cheXzeroCode')\n",
    "\n",
    "from eval import evaluate, bootstrap\n",
    "from zero_shot import make, make_true_labels, run_softmax_eval\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheXpert_data_dir =  r\"D:\\CheXpert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Zero Shot Labels and Templates\n",
    "\n",
    "# ----- DIRECTORIES ------ #\n",
    "cxr_filepath: str = r'C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\cheXzeroCode\\cxr.h5' # filepath of chest x-ray images (.h5)\n",
    "cxr_true_labels_path: Optional[str] = 'D:/CheXpert/groundtruth.csv' # (optional for evaluation) if labels are provided, provide path\n",
    "model_dir: str = 'C:/Users/Vishi/VSC Codes/VIsLM_seminar/VLP-Seminar/data/checkpoints/chexZero checkpoints' # where pretrained models are saved (.pt) \n",
    "predictions_dir: Path = Path('../predictions') # where to save predictions\n",
    "cache_dir: str = predictions_dir / \"cached\" # where to cache ensembled predictions\n",
    "\n",
    "context_length: int = 77\n",
    "\n",
    "# ------- LABELS ------  #\n",
    "# Define labels to query each image | will return a prediction for each label\n",
    "cxr_labels: List[str] = ['Atelectasis','Cardiomegaly', \n",
    "                                      'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "                                      'Lung Opacity', 'No Finding','Pleural Effusion', 'Pleural Other', 'Pneumonia', \n",
    "                                      'Pneumothorax', 'Support Devices']\n",
    "\n",
    "# ---- TEMPLATES ----- # \n",
    "# Define set of templates | see Figure 1 for more details  \n",
    "\n",
    "pathology = \"Cardiomegaly\"                      \n",
    "cxr_pair_template: Tuple[str] = (f\"{pathology}\", f\"no {pathology}\")\n",
    "\n",
    "# ----- MODEL PATHS ------ #\n",
    "# If using ensemble, collect all model paths\n",
    "model_paths = []\n",
    "# print(os.walk(model_dir).__getattribute__)\n",
    "for subdir, dirs, files in os.walk(model_dir):\n",
    "    for file in files:\n",
    "        full_dir = os.path.join(subdir, file)\n",
    "        model_paths.append(full_dir)\n",
    "        \n",
    "print(model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #save all frontal images to a new folder frontal\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define paths\n",
    "# base_folder = r\"D:\\CheXpert\\valid\"  # Replace with the actual path\n",
    "# output_folder = r\"valid_frontal\"  # Replace with the actual path\n",
    "\n",
    "# # Create the output folder if it doesn't exist\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # Copy all frontal images to the output folder\n",
    "# counter = 0\n",
    "# for root, dirs, files in os.walk(base_folder):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\"frontal.jpg\"):\n",
    "\n",
    "#             # get grandparent folder name\n",
    "#             parent_folder = os.path.basename(os.path.dirname(root))\n",
    "#             print(parent_folder)\n",
    "#             newName = f\"{parent_folder}_frontal_{counter}.jpg\"\n",
    "#             img_path = os.path.join(output_folder, newName)\n",
    "#             shutil.copy(os.path.join(root, file), img_path)\n",
    "#             counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert all images in C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\notebooks\\valid_frontal from .jpg to .png\n",
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# # Define paths\n",
    "# base_folder = r\"C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\notebooks\\valid_frontal\"  # Replace with the actual path\n",
    "\n",
    "# # Convert all images to .png\n",
    "# for root, dirs, files in os.walk(base_folder):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".jpg\"):\n",
    "#             img_path = os.path.join(root, file)\n",
    "#             img = Image.open(img_path)\n",
    "#             img.save(img_path.replace(\".jpg\", \".png\"))\n",
    "#             os.remove(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get list of image paths in C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\notebooks\\valid_frontal\n",
    "# import os\n",
    "\n",
    "# # Define paths\n",
    "# base_folder = r\"C:\\Users\\Vishi\\VSC Codes\\VIsLM_seminar\\VLP-Seminar\\notebooks\\valid_frontal\"  # Replace with the actual path\n",
    "\n",
    "# # Get all image paths\n",
    "# image_paths = []\n",
    "# for root, dirs, files in os.walk(base_folder):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".png\"):\n",
    "#             image_paths.append(os.path.join(root, file))\n",
    "\n",
    "# image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocess_padchest import img_to_h5\n",
    "\n",
    "# proper_paths = img_to_h5(image_paths, r\"valid.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cxr_filepath\n",
    "\n",
    "# # If using HDF5 file directly\n",
    "# with h5py.File(cxr_filepath, 'r') as f:\n",
    "#     dataset = f['cxr']\n",
    "#     dataset = torch.tensor(dataset)\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Normalize, Resize, InterpolationMode\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, auc, roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import clip\n",
    "from model import CLIP\n",
    "from eval import evaluate, plot_roc, accuracy, sigmoid, bootstrap, compute_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_softmax_eval(model, loader, eval_labels: list, pair_template: tuple, context_length: int = 77): \n",
    "#     \"\"\"\n",
    "#     Run softmax evaluation to obtain a single prediction from the model.\n",
    "#     \"\"\"\n",
    "#      # get pos and neg phrases\n",
    "#     pos = pair_template[0]\n",
    "#     neg = pair_template[1]\n",
    "\n",
    "#     # get pos and neg predictions, (num_samples, num_classes)\n",
    "#     pos_pred = run_single_prediction(eval_labels, pos, model, loader, \n",
    "#                                      softmax_eval=True, context_length=context_length) \n",
    "#     neg_pred = run_single_prediction(eval_labels, neg, model, loader, \n",
    "#                                      softmax_eval=True, context_length=context_length) \n",
    "\n",
    "#     # compute probabilities with softmax\n",
    "#     sum_pred = np.exp(pos_pred) + np.exp(neg_pred)\n",
    "#     y_pred = np.exp(pos_pred) / sum_pred\n",
    "#     return y_pred\n",
    "# def run_single_prediction(cxr_labels, template, model, loader, softmax_eval=True, context_length=77): \n",
    "#     \"\"\"\n",
    "#     FUNCTION: run_single_prediction\n",
    "#     --------------------------------------\n",
    "#     This function will make probability predictions for a single template\n",
    "#     (i.e. \"has {}\"). \n",
    "    \n",
    "#     args: \n",
    "#         * cxr_labels - list, labels for a specific zero-shot task. (i.e. ['Atelectasis',...])\n",
    "#         * template - string, template to input into model. \n",
    "#         * model - PyTorch model, trained clip model\n",
    "#         * loader - PyTorch data loader, loads in cxr images\n",
    "#         * softmax_eval (optional) - Use +/- softmax method for evaluation \n",
    "#         * context_length (optional) - int, max number of tokens of text inputted into the model.\n",
    "        \n",
    "#     Returns list, predictions from the given template. \n",
    "#     \"\"\"\n",
    "#     cxr_phrase = [template]\n",
    "#     zeroshot_weights = zeroshot_classifier(cxr_labels, cxr_phrase, model, context_length=context_length)\n",
    "#     y_pred = predict(loader, model, zeroshot_weights, softmax_eval=softmax_eval)\n",
    "#     return y_pred\n",
    "\n",
    "# def zeroshot_classifier(classnames, templates, model, context_length=77):\n",
    "#     \"\"\"\n",
    "#     FUNCTION: zeroshot_classifier\n",
    "#     -------------------------------------\n",
    "#     This function outputs the weights for each of the classes based on the \n",
    "#     output of the trained clip model text transformer. \n",
    "    \n",
    "#     args: \n",
    "#     * classnames - Python list of classes for a specific zero-shot task. (i.e. ['Atelectasis',...]).\n",
    "#     * templates - Python list of phrases that will be indpendently tested as input to the clip model.\n",
    "#     * model - Pytorch model, full trained clip model.\n",
    "#     * context_length (optional) - int, max number of tokens of text inputted into the model.\n",
    "    \n",
    "#     Returns PyTorch Tensor, output of the text encoder given templates. \n",
    "#     \"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         zeroshot_weights = []\n",
    "#         # compute embedding through model for each class\n",
    "#         for classname in tqdm(classnames):\n",
    "#             texts = [template.format(classname) for template in templates] # format with class\n",
    "#             texts = clip.tokenize(texts, context_length=context_length) # tokenize\n",
    "#             class_embeddings = model.encode_text(texts) # embed with text encoder\n",
    "            \n",
    "#             # normalize class_embeddings\n",
    "#             class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "#             # average over templates \n",
    "#             class_embedding = class_embeddings.mean(dim=0) \n",
    "#             # norm over new averaged templates\n",
    "#             class_embedding /= class_embedding.norm() \n",
    "#             zeroshot_weights.append(class_embedding)\n",
    "#         zeroshot_weights = torch.stack(zeroshot_weights, dim=1)\n",
    "#     return zeroshot_weights\n",
    "\n",
    "# def predict(loader, model, zeroshot_weights, softmax_eval=True, verbose=0): \n",
    "#     \"\"\"\n",
    "#     FUNCTION: predict\n",
    "#     ---------------------------------\n",
    "#     This function runs the cxr images through the model \n",
    "#     and computes the cosine similarities between the images\n",
    "#     and the text embeddings. \n",
    "    \n",
    "#     args: \n",
    "#         * loader -  PyTorch data loader, loads in cxr images\n",
    "#         * model - PyTorch model, trained clip model \n",
    "#         * zeroshot_weights - PyTorch Tensor, outputs of text encoder for labels\n",
    "#         * softmax_eval (optional) - Use +/- softmax method for evaluation \n",
    "#         * verbose (optional) - bool, If True, will print out intermediate tensor values for debugging.\n",
    "        \n",
    "#     Returns numpy array, predictions on all test data samples. \n",
    "#     \"\"\"\n",
    "#     y_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         for i, data in enumerate(tqdm(loader)):\n",
    "#             images = data['img']\n",
    "#             # predict\n",
    "#             image_features = model.encode_image(images) \n",
    "#             image_features /= image_features.norm(dim=-1, keepdim=True) # (1, 768)\n",
    "\n",
    "#             # obtain logits\n",
    "#             logits = image_features @ zeroshot_weights # (1, num_classes)\n",
    "#             logits = np.squeeze(logits.numpy(), axis=0) # (num_classes,)\n",
    "        \n",
    "#             if softmax_eval is False: \n",
    "#                 norm_logits = (logits - logits.mean()) / (logits.std())\n",
    "#                 logits = sigmoid(norm_logits) \n",
    "            \n",
    "#             y_pred.append(logits)\n",
    "            \n",
    "#             if verbose: \n",
    "#                 plt.imshow(images[0][0])\n",
    "#                 plt.show()\n",
    "#                 print('images: ', images)\n",
    "#                 print('images size: ', images.size())\n",
    "                \n",
    "#                 print('image_features size: ', image_features.size())\n",
    "#                 print('logits: ', logits)\n",
    "#                 print('logits size: ', logits.size())\n",
    "         \n",
    "#     y_pred = np.array(y_pred)\n",
    "#     return np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the model on the data set using ensembled models\n",
    "def ensemble_models(\n",
    "    model_paths: List[str], \n",
    "    cxr_filepath: str, \n",
    "    cxr_labels: List[str], \n",
    "    cxr_pair_template: Tuple[str], \n",
    "    cache_dir: str = None, \n",
    "    save_name: str = None,\n",
    ") -> Tuple[List[np.ndarray], np.ndarray]: \n",
    "    \"\"\"\n",
    "    Given a list of `model_paths`, ensemble model and return\n",
    "    predictions. Caches predictions at `cache_dir` if location provided.\n",
    "\n",
    "    Returns a list of each model's predictions and the averaged\n",
    "    set of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    model_paths = sorted(model_paths) # ensure consistency of \n",
    "    for path in model_paths: # for each model\n",
    "        model_name = Path(path).stem\n",
    "\n",
    "        # load in model and `torch.DataLoader`\n",
    "        model, loader = make(\n",
    "            model_path=path, \n",
    "            cxr_filepath=cxr_filepath, \n",
    "        )\n",
    "        # print(type(loader))  # Should print <class 'torch.utils.data.dataloader.DataLoader'>\n",
    "        # print(type(loader.dataset)) \n",
    "        # print(len(loader))\n",
    "        dataset = loader.dataset\n",
    "        dataset = dataset.img_dset\n",
    "        #print dataset attributes\n",
    "        print(type(dataset))\n",
    "        #get type of dataset content\n",
    "        if cache_dir is not None:\n",
    "            if save_name is not None: \n",
    "                cache_path = Path(cache_dir) / f\"{save_name}_{model_name}.npy\"\n",
    "            else: \n",
    "                cache_path = Path(cache_dir) / f\"{model_name}.npy\"\n",
    "\n",
    "        # if prediction already cached, don't recompute prediction\n",
    "        if cache_dir is not None and os.path.exists(cache_path): \n",
    "            print(\"Loading cached prediction for {}\".format(model_name))\n",
    "            y_pred = np.load(cache_path)\n",
    "        else: # cached prediction not found, compute preds\n",
    "            # print(\"Inferring model {}\".format(path))\n",
    "            # print(\"Model name: {}\".format(model_name))\n",
    "            # print(cxr_labels)\n",
    "            # print(cxr_pair_template)\n",
    "            y_pred = run_softmax_eval(model, loader, cxr_labels, cxr_pair_template)\n",
    "            print(y_pred)\n",
    "            if cache_dir is not None: \n",
    "                Path(cache_dir).mkdir(exist_ok=True, parents=True)\n",
    "                np.save(file=cache_path, arr=y_pred)\n",
    "        predictions.append(y_pred)\n",
    "    \n",
    "    # compute average predictions\n",
    "    y_pred_avg = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return predictions, y_pred_avg\n",
    "\n",
    "predictions, y_pred_avg = ensemble_models(\n",
    "    model_paths=model_paths, \n",
    "    cxr_filepath=cxr_filepath, \n",
    "    cxr_labels=cxr_labels, \n",
    "    cxr_pair_template=cxr_pair_template, \n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save averaged preds\n",
    "pred_name = \"chexpert_preds.npy\" # add name of preds\n",
    "predictions_dir = predictions_dir / pred_name\n",
    "np.save(file=predictions_dir, arr=y_pred_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Evaluate Results\n",
    "If ground truth labels are available, compute AUC on each pathology to evaluate the performance of the zero-shot model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test_true\n",
    "test_pred = y_pred_avg\n",
    "test_true = make_true_labels(cxr_true_labels_path=cxr_true_labels_path, cxr_labels=cxr_labels)\n",
    "\n",
    "test_pred\n",
    "# evaluate model\n",
    "cxr_results = evaluate(test_pred, test_true, cxr_labels)\n",
    "\n",
    "# boostrap evaluations for 95% confidence intervals\n",
    "bootstrap_results = bootstrap(test_pred, test_true, cxr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display AUC with confidence intervals\n",
    "bootstrap_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
